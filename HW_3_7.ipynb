{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tderr24/MAT-422/blob/main/HW_3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54518216",
      "metadata": {
        "id": "54518216"
      },
      "source": [
        "# MAT 422\n",
        "\n",
        "## HW 3.7 - Neural Networks\n",
        "\n",
        "### Thomas Derr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d6bd15",
      "metadata": {
        "id": "69d6bd15"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b0b1e5",
      "metadata": {
        "id": "70b0b1e5"
      },
      "source": [
        "### 3.7.1 Mathematical Formulation\n",
        "\n",
        "In a neural newtowrk, we can create connectons between nodes with weights, and can structure the overall nodemap into a series of layers, with weights connecting nodes from one layer to the next.\n",
        "\n",
        "We can determine the values of a node $j$ in layer $l$ by looking at the output from node $j'$ in layer $l-1$ as follows.\n",
        "\n",
        "$z_{j'}^{(l)}= \\sum \\limits _{j=1}^{J_l-1}w_{j,j'}^{(l)}a_j^{l-1}+b_{j'}^{l}$\n",
        "\n",
        "Given our activation function $\\sigma$, the node values $a_{j'}$ in the next layer are then determined to be\n",
        "\n",
        "$a_{j'}^{(l)} = \\sigma (z_{j'}^{(l)})$\n",
        "\n",
        "\n",
        "### 3.7.2 Activation Functions\n",
        "\n",
        "We can conceptualize our activation function as the thing that adds some element of change to our nodes output. Depending on the value of the node it will change its output in varying ways.\n",
        "\n",
        "There are multiple kinds of activation functions, including\n",
        "\n",
        "Step - $\\sigma(x) = \\begin{equation}\\begin{cases}0, \\quad x<0 \\\\ 1, \\quad x \\geq 0\\end{cases}\\end{equation}$\n",
        "\n",
        "\n",
        "ReLU - $\\sigma(x) = \\max(0,x)$\n",
        "\n",
        "Sigmoid - $\\sigma(x) = \\frac{1}{1+ e^{-x}}$\n",
        "\n",
        "Softmax - $\\frac{e_{z_k}}{\\sum_{k=1}^K e^{z_k}}$\n",
        "\n",
        "\n",
        "### 3.7.3 Cost Function\n",
        "\n",
        "Simmilarly to other models, we use a cost function do measure performance of the network\n",
        "\n",
        "$J = \\frac{1}{2}\\sum \\limits _{n=1}^N \\sum \\limits _{k=1}^K (\\hat{y}_k^{(n)} - y_k^{(n)})^2$\n",
        "\n",
        "\n",
        "### 3.7.4/5 Backpropagation\n",
        "\n",
        "Backpropogation is a method that allows us to tune out weights based on the loss function. If we do this properly, we can effectively minimie the cost function $J$ with respect to the input parameters.\n",
        "\n",
        "### $\\frac{\\partial J}{\\partial w^{(l)}_{j,j'}} = \\frac{\\partial J}{\\partial z^{(l)}_{j'}}\\frac{\\partial z_{j'}^{(l)}}{\\partial w^{(l)}_{j,j'}} = \\delta^{(l)}_{j'}a^{(l-1)}_j$\n",
        "\n",
        "\n",
        "The algorithm for updating the weights and biases is as follows\n",
        "\n",
        "\n",
        "$\\text{New } w^{(l)}_{j,j'} = \\text{Old } w^{(l)}_{j,j'}- \\beta \\frac{\\partial J}{\\partial  w^{(l)}_{j,j'} }$\n",
        "\n",
        "\n",
        "\n",
        "$\\text{New } b^{(l)}_{j'} = \\text{Old } b^{(l)}_{j'}- \\beta \\frac{\\partial J}{\\partial  b^{(l)}_{j'} }$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8b2579",
      "metadata": {
        "id": "3a8b2579",
        "outputId": "83905732-42d2-43aa-87b2-64adb2a9e966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nbgrader/spring22/student-accounts/tderr/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9521276595744681"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.data, df.target, test_size=0.33, random_state=42)\n",
        "\n",
        "clf = MLPClassifier(solver='adam',activation='relu', alpha=1e-5, hidden_layer_sizes=(10, 3))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "yhat = clf.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, yhat)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}